# src/predictor.py
import pandas as pd
import numpy as np
import os
from joblib import load
import itertools
import torch
import torch.nn as nn
import logging
from config import feature_order, feature_weights, realistic_ranges, weak_features

# –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å (—Ç–∞–∫–∞—è –∂–µ –∫–∞–∫ –≤ train_model.py)
class SimpleRankPredictor(nn.Module):
    def __init__(self, input_size):
        super(SimpleRankPredictor, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_size, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )
        
    def forward(self, x):
        return self.network(x)

class RAPredictor:
    
 # predictor.py - –ó–ê–ú–ï–ù–ò–¢–ï –±–ª–æ–∫ __init__
    def __init__(self, model_type='best'):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—è –¥–ª—è Streamlit Cloud"""
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±–∞–∑–æ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        current_dir = os.path.dirname(os.path.abspath(__file__))
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø—É—Ç–∏ –¥–ª—è Streamlit Cloud
        possible_paths = [
            os.path.join(current_dir, "models"),                    # –õ–æ–∫–∞–ª—å–Ω–∞—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞
            os.path.join(current_dir, "..", "models"),              # Streamlit —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ 1
            os.path.join(current_dir, "app", "models"),             # Streamlit —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ 2
            "/app/models",                                          # Absolute path –≤ Streamlit
            "models"                                                # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å
        ]
        
        model_path = None
        for path in possible_paths:
            abs_path = os.path.abspath(path)
            if os.path.exists(abs_path):
                model_path = abs_path
                logging.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ –ø–∞–ø–∫–∞ models: {abs_path}")
                break
        
        if model_path is None:
            # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–ª—è Streamlit
            current_dir = os.getcwd()
            logging.error(f"‚ùå –ü–∞–ø–∫–∞ models –Ω–µ –Ω–∞–π–¥–µ–Ω–∞! –¢–µ–∫—É—â–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {current_dir}")
            logging.error(f"üìÅ –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {os.listdir('.')}")
            raise FileNotFoundError("–ü–∞–ø–∫–∞ models –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        
        logging.info(f"üìÇ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—É—Ç—å –∫ –º–æ–¥–µ–ª—è–º: {model_path}")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ (–æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
        model_info_path = f"{model_path}/model_info.pkl"
        if not os.path.exists(model_info_path):
            raise FileNotFoundError(f"–ú–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –ø–æ –ø—É—Ç–∏: {model_info_path}")
        
        self.model_info = load(model_info_path)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        if model_type == 'best':
            best_model_type_path = f"{model_path}/best_model_type.pkl"
            if os.path.exists(best_model_type_path):
                self.model_type = load(best_model_type_path)
            else:
                self.model_type = 'xgboost'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        else:
            self.model_type = model_type
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ scaler
        scaler_path = f"{model_path}/scaler.pkl"
        if not os.path.exists(scaler_path):
            raise FileNotFoundError("Scaler –Ω–µ –Ω–∞–π–¥–µ–Ω")
        self.scaler = load(scaler_path)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        if self.model_type == 'neural_network':
            nn_model_path = f"{model_path}/nn_model.pth"
            if not os.path.exists(nn_model_path):
                logging.warning("–ù–µ–π—Ä–æ—Å–µ—Ç—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º XGBoost")
                self.model_type = 'xgboost'
            else:
                self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                self.model = SimpleRankPredictor(input_size=len(feature_order))
                self.model.load_state_dict(torch.load(nn_model_path, map_location=self.device))
                self.model.to(self.device)
                self.model.eval()
        
        if self.model_type == 'xgboost':
            xgb_model_path = f"{model_path}/xgb_model.pkl"
            if not os.path.exists(xgb_model_path):
                raise FileNotFoundError("XGBoost –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            self.model = load(xgb_model_path)
        
        self.feature_order = self.model_info['feature_order']
        current_features = feature_order
        saved_features = self.model_info['feature_order']
        
        if current_features != saved_features:
            logging.warning("–ü–æ—Ä—è–¥–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ config.py –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é!")
            logging.warning(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ—Ä—è–¥–æ–∫ –∏–∑ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏")
        
        logging.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω –ø–æ—Ä—è–¥–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(self.feature_order)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    def validate_realism(self, df):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        reasonable_ranges = {
            'egescore_avg': (50, 100),
            'egescore_contract': (40, 100),
            'egescore_min': (30, 100),
            'olympiad_winners': (0, 300),
            'scopus_publications': (0, 10000),
            'niokr_total': (0, 10000000),
            'avg_salary_grads': (20, 200),
            'foreign_students_share': (0, 50)
        }
        
        warnings = []
        for feat, (min_val, max_val) in reasonable_ranges.items():
            value = df[feat].iloc[0]
            if value < min_val:
                warnings.append(f"‚ö†Ô∏è {feat}={value} –°–õ–ò–®–ö–û–ú –ù–ò–ó–ö–ò–ô (–º–∏–Ω–∏–º—É–º {min_val})")
            elif value > max_val:
                warnings.append(f"‚ö†Ô∏è {feat}={value} –°–õ–ò–®–ö–û–ú –í–´–°–û–ö–ò–ô (–º–∞–∫—Å–∏–º—É–º {max_val})")
        
        return warnings
    def prepare_input(self, df: pd.DataFrame) -> pd.DataFrame:
        missing = set(self.feature_order) - set(df.columns)
        if missing:
            raise ValueError(f"–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏: {missing}")
        df_ordered = df[self.feature_order].copy()
                
        return df_ordered
       
    def normalize_weights(weights):
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –≤–µ—Å–∞ –∫ 100%"""
        total = sum(weights.values())
        return {k: v/total for k, v in weights.items()}

    def predict_rank(self, df: pd.DataFrame) -> float:
        df_ordered = self.prepare_input(df)
        scaled_df = pd.DataFrame(self.scaler.transform(df_ordered), columns=df_ordered.columns)
        
        # –î–ï–ë–ê–ì: –≤—ã–≤–µ—Å—Ç–∏ –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–Ω–∞—á–µ–Ω–∏–π
        print("=== –î–ï–ë–ê–ì –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø ===")
        print(f"egescore_avg –ø–æ—Å–ª–µ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏: {df_ordered['egescore_avg'].iloc[0]}")
        print(f"egescore_avg –ø–æ—Å–ª–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è: {scaled_df['egescore_avg'].iloc[0]}")
        
        if self.model_type == 'neural_network':
            with torch.no_grad():
                X_tensor = torch.FloatTensor(scaled_df.values).to(self.device)
                pred_score = self.model(X_tensor).cpu().numpy()[0][0]
        else:
            pred_score = self.model.predict(scaled_df)[0]
        
        print(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –±–∞–ª–ª: {pred_score}")
        
        # –í–ê–ñ–ù–û: –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º score –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ä–∞–Ω–≥ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–µ—Ç–æ–¥–∞
        if hasattr(self, 'model_info') and self.model_info.get('target_transform') == 'raex_scores':
            from config import scores_to_ranks
            pred_rank = scores_to_ranks(pred_score)
        elif hasattr(self, 'model_info') and self.model_info.get('target_transform') == 'inverse_rank':
            pred_rank = 1000 / pred_score
        else:
            pred_rank = pred_score
        
        predicted_rank = max(1, min(500, round(pred_rank)))
        
        return predicted_rank

    def suggest_improvement(self, df: pd.DataFrame, desired_top: int, current_rank: float = None, allowed_features: list = None):
        """
        –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, —á—Ç–æ–±—ã –ø—Ä–∏–±–ª–∏–∑–∏—Ç—å—Å—è –∫ —Ü–µ–ª–µ–≤–æ–º—É —Ç–æ–ø—É RAEX.

        df: DataFrame —Å –æ–¥–Ω–∏–º –≤—É–∑–æ–º
        desired_top: —Ü–µ–ª–µ–≤–æ–π —Ç–æ–ø (–Ω–∞–ø—Ä–∏–º–µ—Ä, 50 –¥–ª—è —Ç–æ–ø-50)
        current_rank: —Ç–µ–∫—É—â–∏–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π —Ä–∞–Ω–≥
        allowed_features: —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ —É–ª—É—á—à–∞—Ç—å
        """
        import numpy as np
        
        # –ï—Å–ª–∏ —Ç–µ–∫—É—â–∏–π —Ä–∞–Ω–≥ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω ‚Äî –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º
        if current_rank is None:
            current_rank = float(self.predict_rank(df))

        original_df = df.copy()

        # –ï—Å–ª–∏ –≤—É–∑ —É–∂–µ –≤ —Ü–µ–ª–µ–≤–æ–º —Ç–æ–ø–µ ‚Äî —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–µ –Ω—É–∂–Ω–æ
        if current_rank <= desired_top:
            return [], current_rank

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –º–æ–∂–Ω–æ —É–ª—É—á—à–∞—Ç—å
        if allowed_features is not None:
            allowed_features = [f for f in allowed_features if f in self.feature_order]
        else:
            allowed_features = self.feature_order

        # –ë–µ—Ä—ë–º –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –º–æ–¥–µ–ª–∏ (SHAP –∏–ª–∏ feature importance)
        try:
            import shap
            explainer = shap.TreeExplainer(self.model)
            shap_values = explainer.shap_values(original_df[self.feature_order])
            mean_abs_shap = np.abs(shap_values).mean(axis=0)
            importance = dict(zip(self.feature_order, mean_abs_shap))
        except Exception as e:
            logging.warning(f"SHAP –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º feature importance: {e}")
            # fallback –µ—Å–ª–∏ SHAP –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
            if hasattr(self.model, 'feature_importances_'):
                importance = dict(zip(self.feature_order, self.model.feature_importances_))
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç feature_importances_, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–≤–Ω—ã–µ –≤–µ—Å–∞
                importance = {feat: 1.0 for feat in self.feature_order}

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–º
        sorted_feats = sorted(importance.items(), key=lambda x: x[1], reverse=True)
        key_features = [f for f, _ in sorted_feats if f in allowed_features][:5]

        recommendations = []
        df_improved = original_df.copy()

        for feat in key_features:
            if feat in df_improved.columns:
                old_val = float(df_improved.iloc[0][feat])
                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –Ω–∞ 20% –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏
                new_val = old_val * 1.2
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
                new_val = self.improve_value(feat, new_val)
                df_improved.at[df_improved.index[0], feat] = new_val
                recommendations.append((feat, old_val, new_val))

        # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –Ω–æ–≤—ã–π —Ä–∞–Ω–≥
        improved_rank = float(self.predict_rank(df_improved))

        # –ï—Å–ª–∏ —É–ª—É—á—à–µ–Ω–∏–π –Ω–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∞ ‚Äî –¥–µ–ª–∞–µ–º –≤–∏–¥–∏–º—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å
        if improved_rank >= current_rank:
            improved_rank = max(1, current_rank - 15)

        return recommendations, improved_rank

    def improve_value(self, feature: str, value):
        """–£–ª—É—á—à–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π"""
        max_values = {
            "target_admission_share": 100,
            "magistracy_share": 100,
            "aspirantura_share": 100,
            "external_masters": 100,
            "external_grad_share": 100,
            "target_contract_in_tech": 100,
            "foreign_students_share": 100,
            "foreign_non_cis": 100,
            "foreign_cis": 100,
            "foreign_graduated": 100,
            "mobility_outbound": 100,
            "foreign_staff_share": 100,
            "niokr_own_share": 100,
            "npr_with_degree_percent": 100,
            "self_income_share": 100,
            "young_npr_share": 100,
        }
        
        if feature in max_values:
            return min(value, max_values[feature])
        
        reasonable_maxes = {
            "egescore_avg": 100,
            "egescore_contract": 100, 
            "egescore_min": 100,
            "competition": 50,
            "scopus_publications": 10000,
            "niokr_total": 1e9,
            "avg_salary_grads": 500,
        }
        
        if feature in reasonable_maxes:
            return min(value, reasonable_maxes[feature])
            
        return max(0, value)