# src/train_model.py
import pandas as pd
import glob
import os
import numpy as np
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
from sklearn.preprocessing import StandardScaler, RobustScaler
from xgboost import XGBRegressor
from joblib import dump
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import logging

from config import feature_order, feature_weights, realistic_ranges, weak_features

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SimpleRankPredictor(nn.Module):
    def __init__(self, input_size):
        super(SimpleRankPredictor, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_size, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )
        
    def forward(self, x):
        return self.network(x)

def add_synthetic_universities(df, n_top=200, n_mid=300, n_low=200):
    """–°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ"""
    synthetic_data = []
    
    # –¢–û–ü-–≤—É–∑—ã (—Ä–∞–Ω–≥–∏ 1-50) - –õ–£–ß–®–ò–ï
    for i in range(n_top):
        synthetic_uni = {
            'egescore_avg': np.random.uniform(80, 95),
            'olympiad_winners': np.random.randint(50, 300),
            'olympiad_other': np.random.randint(100, 400),
            'competition': np.random.uniform(5.0, 15.0),
            'scopus_publications': np.random.randint(1000, 5000),
            'niokr_total': np.random.uniform(5000000, 15000000),
            'avg_salary_grads': np.random.uniform(80000, 150000),
            'total_income_per_student': np.random.uniform(800000, 2000000),
            'foreign_students_share': np.random.uniform(8.0, 25.0),
            'rank': np.random.randint(1, 51)  # –¢–û–õ–¨–ö–û 1-50!
        }
        synthetic_uni = complete_synthetic_data(synthetic_uni, "top")
        synthetic_data.append(synthetic_uni)
    
    # –°—Ä–µ–¥–Ω–∏–µ –≤—É–∑—ã (—Ä–∞–Ω–≥–∏ 51-200)
    for i in range(n_mid):
        synthetic_uni = {
            'egescore_avg': np.random.uniform(65, 80),
            'olympiad_winners': np.random.randint(5, 50),
            'olympiad_other': np.random.randint(20, 100),
            'competition': np.random.uniform(2.0, 6.0),
            'scopus_publications': np.random.randint(100, 1000),
            'niokr_total': np.random.uniform(500000, 3000000),
            'avg_salary_grads': np.random.uniform(50000, 80000),
            'total_income_per_student': np.random.uniform(300000, 800000),
            'foreign_students_share': np.random.uniform(2.0, 10.0),
            'rank': np.random.randint(51, 201)  # –¢–û–õ–¨–ö–û 51-200!
        }
        synthetic_uni = complete_synthetic_data(synthetic_uni, "mid")
        synthetic_data.append(synthetic_uni)
    
    # –°–ª–∞–±—ã–µ –≤—É–∑—ã (—Ä–∞–Ω–≥–∏ 201-500) - –•–£–î–®–ò–ï
    for i in range(n_low):
        synthetic_uni = {
            'egescore_avg': np.random.uniform(45, 60),
            'olympiad_winners': np.random.randint(0, 5),
            'olympiad_other': np.random.randint(0, 10),
            'competition': np.random.uniform(0.1, 2.0),
            'scopus_publications': np.random.randint(0, 50),
            'niokr_total': np.random.uniform(0, 200000),
            'avg_salary_grads': np.random.uniform(20000, 40000),
            'total_income_per_student': np.random.uniform(50000, 200000),
            'foreign_students_share': np.random.uniform(0.0, 2.0),
            'rank': np.random.randint(201, 501)  # –¢–û–õ–¨–ö–û 201-500!
        }
        synthetic_uni = complete_synthetic_data(synthetic_uni, "low")
        synthetic_data.append(synthetic_uni)
    
    return pd.concat([df, pd.DataFrame(synthetic_data)], ignore_index=True)

def complete_synthetic_data(base_data, tier):
    """–ó–∞–ø–æ–ª–Ω—è–µ—Ç –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç tier"""
    defaults = {
        "top": {
            'egescore_contract': np.random.uniform(70, 85),
            'egescore_min': np.random.uniform(60, 75),
            'niokr_per_npr': np.random.uniform(500, 1500),
            'risc_citations': np.random.randint(2000, 5000),
            'risc_publications': np.random.randint(100, 300),
            'self_income_per_npr': np.random.uniform(1500, 3000),
            'ppc_salary_index': np.random.uniform(150, 250),
            'foreign_non_cis': np.random.uniform(5.0, 15.0),
            'foreign_edu_income': np.random.uniform(300000, 1000000),
            'npr_with_degree_percent': np.random.uniform(75, 90),
            'area_per_student': np.random.uniform(15, 30)
        },
        "mid": {
            'egescore_contract': np.random.uniform(60, 75),
            'egescore_min': np.random.uniform(50, 65),
            'niokr_per_npr': np.random.uniform(200, 600),
            'risc_citations': np.random.randint(500, 2000),
            'risc_publications': np.random.randint(50, 150),
            'self_income_per_npr': np.random.uniform(800, 1800),
            'ppc_salary_index': np.random.uniform(120, 180),
            'foreign_non_cis': np.random.uniform(1.0, 6.0),
            'foreign_edu_income': np.random.uniform(50000, 300000),
            'npr_with_degree_percent': np.random.uniform(65, 80),
            'area_per_student': np.random.uniform(10, 20)
        },
        "low": {
            'egescore_contract': np.random.uniform(50, 65),
            'egescore_min': np.random.uniform(40, 55),
            'niokr_per_npr': np.random.uniform(50, 250),
            'risc_citations': np.random.randint(0, 500),
            'risc_publications': np.random.randint(0, 50),
            'self_income_per_npr': np.random.uniform(200, 900),
            'ppc_salary_index': np.random.uniform(100, 140),
            'foreign_non_cis': np.random.uniform(0.0, 2.0),
            'foreign_edu_income': np.random.uniform(0, 50000),
            'npr_with_degree_percent': np.random.uniform(55, 70),
            'area_per_student': np.random.uniform(5, 15)
        }
    }
    
    for feat, value_range in defaults[tier].items():
        if feat not in base_data:
            if isinstance(value_range, tuple):
                base_data[feat] = np.random.uniform(value_range[0], value_range[1])
            else:
                base_data[feat] = value_range
    
    # –ó–∞–ø–æ–ª–Ω—è–µ–º —Å–ª–∞–±—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    for feat in weak_features:
        if feat not in base_data:
            base_data[feat] = np.random.uniform(0, 10)  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    
    return base_data
def transform_target_variable(y):
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ä–∞–Ω–≥–∏ –≤ –±–∞–ª–ª—ã RAEX (0-100)"""
    from config import transform_to_raex_scores
    return transform_to_raex_scores(y)

def inverse_transform_target(y_pred):
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –±–∞–ª–ª—ã –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ä–∞–Ω–≥–∏ (1-500)"""
    from config import scores_to_ranks
    return scores_to_ranks(y_pred)

def train_pytorch_model(X_train, y_train, X_val, y_val, epochs=500, patience=30):
    """–û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ PyTorch —Å —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–æ–π"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    input_size = X_train.shape[1]
    model = SimpleRankPredictor(input_size).to(device)
    
    def init_weights(m):
        if isinstance(m, nn.Linear):
            nn.init.xavier_uniform_(m.weight)
            nn.init.constant_(m.bias, 0)
    
    model.apply(init_weights)
    
    criterion = nn.HuberLoss(delta=1.0)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)
    
    X_train_tensor = torch.FloatTensor(X_train).to(device)
    y_train_tensor = torch.FloatTensor(y_train.values).reshape(-1, 1).to(device)
    X_val_tensor = torch.FloatTensor(X_val).to(device)
    y_val_tensor = torch.FloatTensor(y_val.values).reshape(-1, 1).to(device)
    
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    
    best_val_loss = float('inf')
    best_model_state = None
    patience_counter = 0
    
    for epoch in range(epochs):
        model.train()
        train_loss = 0
        for batch_X, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            
            if torch.isnan(loss):
                continue
                
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            train_loss += loss.item()
        
        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val_tensor)
            val_loss = criterion(val_outputs, y_val_tensor).item()
        
        if np.isnan(val_loss):
            continue
            
        scheduler.step(val_loss)
        
        if epoch % 50 == 0:
            logger.info(f'Epoch {epoch}: Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}')
        
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model_state = model.state_dict().copy()
            patience_counter = 0
        else:
            patience_counter += 1
            
        if patience_counter >= patience:
            logger.info(f'–†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ —ç–ø–æ—Ö–µ {epoch}')
            break
    
    if best_model_state is None:
        best_model_state = model.state_dict().copy()
    
    model.load_state_dict(best_model_state)
    return model

def load_and_preprocess_data(data_folder="data"):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
    df_list = []
    for file in glob.glob(f"{data_folder}/raex_*.csv"):
        df = pd.read_csv(file)
        if "year" not in df.columns:
            year = int(file.split("_")[-1].split(".")[0])
            df["year"] = year
        df_list.append(df)
    
    raw_df = pd.concat(df_list, ignore_index=True)
    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö: {len(raw_df)} –∑–∞–ø–∏—Å–µ–π")
    logger.info(f"–ì–æ–¥—ã –≤ –¥–∞–Ω–Ω—ã—Ö: {sorted(raw_df['year'].unique())}")
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
    logger.info("–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤—É–∑–æ–≤...")
    raw_df = add_synthetic_universities(raw_df)
    logger.info(f"–ü–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö: {len(raw_df)} –∑–∞–ø–∏—Å–µ–π")
    
    return raw_df

def prepare_features(df):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ë–ï–ó –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –≤–µ—Å–æ–≤"""
    X = df[feature_order].copy()
    
    # –¢–û–õ–¨–ö–û –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤ –∏ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ NaN, –ë–ï–ó –≤–µ—Å–æ–≤!
    for feat in feature_order:
        upper_limit = X[feat].quantile(0.95)
        if upper_limit > 0:
            X[feat] = np.where(X[feat] > upper_limit, upper_limit, X[feat])
    
    X = X.fillna(X.median())
    
    return X

def transform_to_raex_scores(y):
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ä–∞–Ω–≥–∏ –≤ –±–∞–ª–ª—ã –ø–æ –º–µ—Ç–æ–¥–∏–∫–µ RAEX"""
    # RAEX: –ª—É—á—à–∏–π –≤—É–∑ = ~100 –±–∞–ª–ª–æ–≤, —Ö—É–¥—à–∏–π = ~0 –±–∞–ª–ª–æ–≤
    max_rank = 500
    min_rank = 1
    
    # –õ–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ: —Ä–∞–Ω–≥ 1 -> 100 –±–∞–ª–ª–æ–≤, —Ä–∞–Ω–≥ 500 -> 0 –±–∞–ª–ª–æ–≤
    scores = 100 * (max_rank - y) / (max_rank - min_rank)
    return scores
def test_predictions(model, scaler):
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""
    logger.info("=== –¢–ï–°–¢ –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø ===")
    
    test_cases = {
        '–¢–æ–ø-–≤—É–∑ (–ú–ì–£)': create_test_case("top"),
        '–°—Ä–µ–¥–Ω–∏–π –≤—É–∑': create_test_case("mid"), 
        '–°–ª–∞–±—ã–π –≤—É–∑': create_test_case("low")
    }
    
    for name, test_data in test_cases.items():
        test_df = pd.DataFrame([test_data])
        test_df = test_df[feature_order]  # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫
        test_scaled = scaler.transform(test_df)
        predicted_score = model.predict(test_scaled)[0]
        predicted_rank = inverse_transform_target(predicted_score)
        logger.info(f"{name}: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π —Ä–∞–Ω–≥ {predicted_rank:.1f} (–±–∞–ª–ª: {predicted_score:.1f})")

def test_real_universities(model, scaler):
    """–¢–µ—Å—Ç –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞—Ö"""
    logger.info("=== –¢–ï–°–¢ –†–ï–ê–õ–¨–ù–´–• –í–£–ó–û–í ===")
    
    # –î–∞–Ω–Ω—ã–µ –ú–ì–¢–£ –∏–º. –ë–∞—É–º–∞–Ω–∞ (2023)
    bmstu_data = {
        'egescore_avg': 80.83, 'egescore_contract': 71.98, 'egescore_min': 54.55,
        'olympiad_winners': 8, 'olympiad_other': 236, 'competition': 5.0,
        'target_admission_share': 13.59, 'target_contract_in_tech': 20.37,
        'magistracy_share': 10.30, 'aspirantura_share': 2.70,
        'external_masters': 98.72, 'external_grad_share': 47.70,
        'aspirants_per_100_students': 3.70,
        'foreign_students_share': 5.71, 'foreign_non_cis': 3.70, 'foreign_cis': 2.01,
        'foreign_graduated': 7.66, 'mobility_outbound': 0.07,
        'foreign_staff_share': 0.22, 'foreign_professors': 0,
        'niokr_total': 3982904.40, 'niokr_share_total': 22.40, 'niokr_own_share': 84.29,
        'niokr_per_npr': 1919.01, 'scopus_publications': 160.44, 'risc_publications': 160.44,
        'risc_citations': 409.68, 'foreign_niokr_income': 0.00, 'journals_published': 13,
        'grants_per_100_npr': 2.84,
        'foreign_edu_income': 31664.10, 'total_income_per_student': 827.28,
        'self_income_per_npr': 1939.98, 'self_income_share': 22.59,
        'ppc_salary_index': 200.57, 'avg_salary_grads': 100.0,
        'npr_with_degree_percent': 62.89, 'npr_per_100_students': 5.77,
        'young_npr_share': 13.63, 'lib_books_per_student': 106.41,
        'area_per_student': 10.36, 'pc_per_student': 0.36
    }
    
    test_df = pd.DataFrame([bmstu_data])
    test_df = test_df[feature_order]
    test_scaled = scaler.transform(test_df)
    predicted_score = model.predict(test_scaled)[0]
    predicted_rank = inverse_transform_target(predicted_score)
    
    logger.info(f"üéØ –ú–ì–¢–£ –∏–º. –ë–∞—É–º–∞–Ω–∞: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π —Ä–∞–Ω–≥ {predicted_rank:.1f} (–±–∞–ª–ª: {predicted_score:.1f})")
    logger.info(f"üìä –†–µ–∞–ª—å–Ω—ã–π —Ä–∞–Ω–≥ –ë–∞—É–º–∞–Ω–∫–∏: 2 –º–µ—Å—Ç–æ")
def scores_to_ranks(scores):
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –±–∞–ª–ª—ã –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ä–∞–Ω–≥–∏"""
    max_rank = 500
    min_rank = 1
    ranks = max_rank - (scores * (max_rank - min_rank) / 100)
    return ranks.round().astype(int)

def train_and_save_models(data_folder="data", model_folder="models"):
    """–û–±—É—á–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å –±–∞–ª–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π"""
    os.makedirs(model_folder, exist_ok=True)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    raw_df = load_and_preprocess_data(data_folder)
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    X = prepare_features(raw_df)
    y = raw_df["rank"]
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –≤ –ë–ê–õ–õ–´ (0-100)
    y_transformed = transform_target_variable(y)
    
    # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    logger.info("=== –°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–ê–ù–ù–´–• ===")
    logger.info(f"–ó–∞–ø–∏—Å–µ–π: {len(raw_df)}")
    logger.info(f"–î–∏–∞–ø–∞–∑–æ–Ω —Ä–∞–Ω–≥–æ–≤: {y.min()} - {y.max()}")
    logger.info(f"–î–∏–∞–ø–∞–∑–æ–Ω –±–∞–ª–ª–æ–≤: {y_transformed.min():.1f} - {y_transformed.max():.1f}")
    logger.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–Ω–≥–æ–≤:")
    logger.info(f"–¢–æ–ø-50: {len(y[y <= 50])} –≤—É–∑–æ–≤")
    logger.info(f"51-200: {len(y[(y > 50) & (y <= 200)])} –≤—É–∑–æ–≤") 
    logger.info(f"201-500: {len(y[y > 200])} –≤—É–∑–æ–≤")
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val
    X_train, X_val, y_train, y_val = train_test_split(
        X, y_transformed, test_size=0.2, random_state=42, shuffle=True
    )
    
    # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    scaler = RobustScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    
    logger.info("=== –û–ë–£–ß–ï–ù–ò–ï XGBOOST (–±–∞–ª–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞) ===")
    xgb_model = XGBRegressor(
        n_estimators=1000,
        learning_rate=0.1,
        max_depth=6,
        subsample=0.8,
        colsample_bytree=0.8,
        reg_alpha=2.0,
        reg_lambda=2.0,
        random_state=42,
        early_stopping_rounds=50
    )
    
    xgb_model.fit(
        X_train_scaled, y_train,
        eval_set=[(X_val_scaled, y_val)],
        verbose=100
    )
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –æ–±—Ä–∞—Ç–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –†–ê–ù–ì–ò
    y_pred_train_scores = xgb_model.predict(X_train_scaled)
    y_pred_val_scores = xgb_model.predict(X_val_scaled)
    
    y_pred_train = inverse_transform_target(y_pred_train_scores)
    y_pred_val = inverse_transform_target(y_pred_val_scores)
    
    # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–∞–Ω–≥–∏ –¥–ª—è –º–µ—Ç—Ä–∏–∫
    y_train_orig = inverse_transform_target(y_train)
    y_val_orig = inverse_transform_target(y_val)
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    train_mae = mean_absolute_error(y_train_orig, y_pred_train)
    train_r2 = r2_score(y_train_orig, y_pred_train)
    val_mae = mean_absolute_error(y_val_orig, y_pred_val)
    val_r2 = r2_score(y_val_orig, y_pred_val)
    
    logger.info(f"–û–ë–£–ß–ï–ù–ò–ï - MAE: {train_mae:.3f}, R¬≤: {train_r2:.3f}")
    logger.info(f"–í–ê–õ–ò–î–ê–¶–ò–Ø - MAE: {val_mae:.3f}, R¬≤: {val_r2:.3f}")
    
    # –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º
    errors = y_pred_val - y_val_orig
    top_50_error = errors[y_val_orig <= 50].mean()
    mid_error = errors[(y_val_orig > 50) & (y_val_orig <= 200)].mean()
    low_error = errors[y_val_orig > 200].mean()
    
    logger.info(f"–û—à–∏–±–∫–∞ –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º:")
    logger.info(f"–¢–æ–ø-50: {top_50_error:.3f}")
    logger.info(f"51-200: {mid_error:.3f}")
    logger.info(f"201-500: {low_error:.3f}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    dump(xgb_model, f"{model_folder}/xgb_model.pkl")
    dump(scaler, f"{model_folder}/scaler.pkl")
    
    # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_importance = pd.DataFrame({
        'feature': feature_order,
        'importance': xgb_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    logger.info("–¢–æ–ø-10 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    for i, row in feature_importance.head(10).iterrows():
        logger.info(f"{row['feature']}: {row['importance']:.4f}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –º–µ—Ç–æ–¥–∞
    model_info = {
        'feature_order': feature_order,
        'feature_weights': feature_weights,
        'training_years': sorted(raw_df['year'].unique()),
        'data_shape': X.shape,
        'performance': {
            'xgboost': {
                'train_mae': train_mae, 'train_r2': train_r2,
                'val_mae': val_mae, 'val_r2': val_r2
            }
        },
        'feature_importance': feature_importance.to_dict('records'),
        'target_transform': 'raex_scores',  # –£–∫–∞–∑—ã–≤–∞–µ–º –º–µ—Ç–æ–¥
        'target_stats': {
            'min_rank': y.min(),
            'max_rank': y.max(),
            'mean_rank': y.mean(),
            'min_score': y_transformed.min(),
            'max_score': y_transformed.max()
        },
        'model_type': 'xgboost'
    }
    
    dump(model_info, f"{model_folder}/model_info.pkl")
    dump('xgboost', f"{model_folder}/best_model_type.pkl")
    # –ü–æ—Å–ª–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è model_info –¥–æ–±–∞–≤—å:
    logger.info("=== –ü–†–û–í–ï–†–ö–ê –í–ï–°–û–í ===")
    logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –≤–µ—Å–∞: {len(feature_weights)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    top_weights = sorted(feature_weights.items(), key=lambda x: x[1], reverse=True)[:5]
    for feat, weight in top_weights:
        logger.info(f"  {feat}: {weight:.4f}")
    logger.info("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞! (–±–∞–ª–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞)")
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–∞—Ö –≤—É–∑–æ–≤
    test_predictions(xgb_model, scaler)
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Ç–µ—Å—Ç –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    test_real_universities(xgb_model, scaler)
    # –í –∫–æ–Ω—Ü–µ train_and_save_models –ø–µ—Ä–µ–¥ return
    import joblib
    saved_info = joblib.load(f"{model_folder}/model_info.pkl")
    print("=== –ü–†–û–í–ï–†–ö–ê –§–ê–ô–õ–ê model_info.pkl ===")
    print(f"–í–µ—Å–∞ –≤ —Ñ–∞–π–ª–µ: {len(saved_info['feature_weights'])} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    
def create_test_case(tier):
    """–°–æ–∑–¥–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ tier —Å–æ –≤—Å–µ–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏"""
    base_data = {
        'egescore_avg': 0.0, 'egescore_contract': 0.0, 'egescore_min': 0.0,
        'olympiad_winners': 0, 'olympiad_other': 0, 'competition': 0.0,
        'target_admission_share': 0.0, 'magistracy_share': 0.0, 'aspirantura_share': 0.0,
        'external_masters': 0.0, 'external_grad_share': 0.0, 'aspirants_per_100_students': 0.0,
        'target_contract_in_tech': 0.0, 'foreign_students_share': 0.0, 'foreign_non_cis': 0.0,
        'foreign_cis': 0.0, 'foreign_graduated': 0.0, 'mobility_outbound': 0.0,
        'foreign_staff_share': 0.0, 'foreign_professors': 0, 'niokr_total': 0.0,
        'niokr_share_total': 0.0, 'niokr_own_share': 0.0, 'niokr_per_npr': 0.0,
        'npr_with_degree_percent': 0.0, 'npr_per_100_students': 0.0, 'ppc_salary_index': 0.0,
        'avg_salary_grads': 0.0, 'scopus_publications': 0, 'risc_publications': 0.0,
        'risc_citations': 0.0, 'foreign_niokr_income': 0.0, 'foreign_edu_income': 0.0,
        'total_income_per_student': 0.0, 'self_income_per_npr': 0.0, 'self_income_share': 0.0,
        'lib_books_per_student': 0.0, 'area_per_student': 0.0, 'pc_per_student': 0.0,
        'young_npr_share': 0.0, 'journals_published': 0, 'grants_per_100_npr': 0.0
    }
    
    if tier == "top":
        # –¢–æ–ø-–≤—É–∑ (–ú–ì–£)
        base_data.update({
            'egescore_avg': 80.0, 'egescore_contract': 65.0, 'egescore_min': 60.0,
            'olympiad_winners': 220, 'olympiad_other': 230, 'competition': 7.7,
            'target_admission_share': 1.8, 'magistracy_share': 28.0, 'aspirantura_share': 35.0,
            'external_masters': 48.0, 'external_grad_share': 64.0, 'aspirants_per_100_students': 11.0,
            'target_contract_in_tech': 1.2, 'foreign_students_share': 15.8, 'foreign_non_cis': 13.4,
            'foreign_cis': 1.5, 'foreign_graduated': 19.5, 'mobility_outbound': 0.5,
            'foreign_staff_share': 0.8, 'foreign_professors': 75, 'niokr_total': 9500000,
            'niokr_share_total': 23.0, 'niokr_own_share': 99.0, 'niokr_per_npr': 690.0,
            'npr_with_degree_percent': 82.0, 'npr_per_100_students': 17.0, 'ppc_salary_index': 185.0,
            'avg_salary_grads': 105000, 'scopus_publications': 3200, 'risc_publications': 150,
            'risc_citations': 3100, 'foreign_niokr_income': 25000, 'foreign_edu_income': 800000,
            'total_income_per_student': 1080000, 'self_income_per_npr': 2120.0, 'self_income_share': 42.0,
            'lib_books_per_student': 250, 'area_per_student': 22.0, 'pc_per_student': 0.65,
            'young_npr_share': 19.0, 'journals_published': 70, 'grants_per_100_npr': 10.5
        })
    elif tier == "mid":
        # –°—Ä–µ–¥–Ω–∏–π –≤—É–∑
        base_data.update({
            'egescore_avg': 70.0, 'egescore_contract': 60.0, 'egescore_min': 50.0,
            'olympiad_winners': 20, 'olympiad_other': 50, 'competition': 3.0,
            'target_admission_share': 5.0, 'magistracy_share': 20.0, 'aspirantura_share': 15.0,
            'external_masters': 35.0, 'external_grad_share': 45.0, 'aspirants_per_100_students': 5.0,
            'target_contract_in_tech': 1.0, 'foreign_students_share': 5.0, 'foreign_non_cis': 3.0,
            'foreign_cis': 2.0, 'foreign_graduated': 8.0, 'mobility_outbound': 0.2,
            'foreign_staff_share': 1.0, 'foreign_professors': 10, 'niokr_total': 1000000,
            'niokr_share_total': 15.0, 'niokr_own_share': 90.0, 'niokr_per_npr': 300.0,
            'npr_with_degree_percent': 70.0, 'npr_per_100_students': 10.0, 'ppc_salary_index': 130.0,
            'avg_salary_grads': 60000, 'scopus_publications': 300, 'risc_publications': 80,
            'risc_citations': 800, 'foreign_niokr_income': 5000, 'foreign_edu_income': 100000,
            'total_income_per_student': 500000, 'self_income_per_npr': 1000.0, 'self_income_share': 30.0,
            'lib_books_per_student': 150, 'area_per_student': 15.0, 'pc_per_student': 0.5,
            'young_npr_share': 15.0, 'journals_published': 10, 'grants_per_100_npr': 5.0
        })
    else:  # low
        # –°–ª–∞–±—ã–π –≤—É–∑
        base_data.update({
            'egescore_avg': 48.0, 'egescore_contract': 45.0, 'egescore_min': 40.0,
            'olympiad_winners': 0, 'olympiad_other': 1, 'competition': 0.3,
            'target_admission_share': 10.0, 'magistracy_share': 10.0, 'aspirantura_share': 5.0,
            'external_masters': 20.0, 'external_grad_share': 30.0, 'aspirants_per_100_students': 1.0,
            'target_contract_in_tech': 0.5, 'foreign_students_share': 1.0, 'foreign_non_cis': 0.5,
            'foreign_cis': 0.5, 'foreign_graduated': 2.0, 'mobility_outbound': 0.05,
            'foreign_staff_share': 0.2, 'foreign_professors': 2, 'niokr_total': 50000,
            'niokr_share_total': 5.0, 'niokr_own_share': 80.0, 'niokr_per_npr': 100.0,
            'npr_with_degree_percent': 60.0, 'npr_per_100_students': 5.0, 'ppc_salary_index': 100.0,
            'avg_salary_grads': 35000, 'scopus_publications': 10, 'risc_publications': 20,
            'risc_citations': 100, 'foreign_niokr_income': 0, 'foreign_edu_income': 10000,
            'total_income_per_student': 150000, 'self_income_per_npr': 300.0, 'self_income_share': 15.0,
            'lib_books_per_student': 80, 'area_per_student': 8.0, 'pc_per_student': 0.3,
            'young_npr_share': 10.0, 'journals_published': 1, 'grants_per_100_npr': 1.0
        })
    
    return base_data

    
if __name__ == "__main__":
    train_and_save_models()